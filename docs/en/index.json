[
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/",
	"title": "Analytics on AWS",
	"tags": [],
	"description": "",
	"content": "Analytics on AWS The purpose of this lab is to implement the Businesss Intelligence System using AWS Analytics Services. Through this lab, the steps of Data Collection -\u0026gt; Storage -\u0026gt; Analysis/Processing -\u0026gt; Visualization are performed using AWS Analytics Services. You can experience how you can build it.\n"
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Analytics on AWS Through this lab, the steps of Data Collection -\u0026gt; Storage -\u0026gt; Analysis/Processing -\u0026gt; Visualization are performed using AWS Analytics Services. You can experience how you can build it.\nIn this lab, we are going to implement the following Businesss Intelligence System (BI System) using AWS Analytics Services.\n"
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/lab-setup/",
	"title": "Lab setup",
	"tags": [],
	"description": "",
	"content": " If you already have an AWS account, you can immediately follow this lab guide. Otherwise you must first create an AWS account.\n To create and activate an AWS account, please refer to the following link .\n 2-1. Creating an IAM User  2-2. Creating Security Groups  2-3. Launch an EC2 Instance  2-4. Configuring your EC2 Instance   "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/lab-setup/iam-user/",
	"title": "Creating an IAM User",
	"tags": [],
	"description": "",
	"content": " Let\u0026rsquo;s create an IAM User to use during the lab.\n  Log in to the AWS Management Console and access the IAM service. Select Users from the left menu. Click the Add user button to enter the Add User page. Enter \u0026lt;user name\u0026gt; in User name, and then choose both Programmatic access and AWS Management Console access. Next, enter \u0026lt;password\u0026gt; in Console password, In last, uncheck Require password reset. Click the [Next: Permissions] button, select Attach existing policies directly, and add AdministratorAccess privileges. Click the [Next: Review] button, check the information, and click the Create user button. Click the Download.csv button to download the new user\u0026rsquo;s information. This file is essential for setting up EC2, so save it in a location that is easy to remember.  "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/lab-setup/ec2-security-group/",
	"title": "Creating Security Groups",
	"tags": [],
	"description": "",
	"content": "Security Groups to create an EC2 instance for a bastion host Create and configure a security group of EC2 instance.\n  Connect to the EC2 service in the AWS Management Console.\n  Select the Security Groups item from the NETWORK \u0026amp; SECURITY menu.\n  Click [Create Security Group].\n  On the Create Security Group screen, enter the necessary information for the Security Group, and then [Create] a new security group.\n Security group name : bastion Description : security group for bastion  Security group rules의 Inbound 에 아래 내용을 입력합니다.\n Type : SSH Protocol : TCP Port Range : 22 Source : 0.0.0.0/0    Security Groups created for use in Elasticsearch Service Create and configure a security group for Elasticsearch Service.\n  Connect to EC2 service in AWS Management Console.\n  Select the Security Groups item from the NETWORK \u0026amp; SECURITY menu.\n  Click [Create Security Group].\n  On the Create Security Group screen, enter the necessary information for the Security Group, and then [Create] a new security group.\n Security group name : use-es-cluster-sg Description : security group for an es client  Enter nothing in Inbound of the security group rules.\n  Click [Create Security Group] again to go to the Create Security Group screen. After entering the necessary information for the security group, [Create] a new security group.\n Security group name : es-cluster-sg Description : security group for an es cluster  Enter the following in Inbound of the security group rules.\n Type : All TCP Protocol : TCP Port Range : 0-65535 Source : use-es-cluster-sg 의 security group id ex) sg-038b632ef1825cb7f    "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/lab-setup/ec2-launch/",
	"title": "Launch an EC2 Instance",
	"tags": [],
	"description": "",
	"content": " For this lab, we will use the us-west-1 (Oregon) region.\n Create an EC2 instance that will generate the data needed for the lab in real time.\n Connect to EC2 service in AWS Management Console. In the upper right, select US West (Oregon) for Region. Select Instances from the left INSTANCES menu and click [Launch Instance] to start creating a new instance.  Step 1: On the Choose an Amazon Machine Image (AMI) screen, choose Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type.  Step 2: On the Choose an Instance Type screen, select t2.micro as the instance type. Click [Next: Configure Instance Details].  Step 3: On the Configure Instance Details screen, select Enable for Auto-assign Public IP, and click [Next: Add Storage].  Step 4: On the Add Storage screen, leave the defaults and click [Next: Add Tags]. Step 5: On the Add Tags screen, click [Next: Configure Security Group]. Step 6: On the Configure Security Group screen, select Select an existing security group from Assign a security group, and then select bastion and use-es-cluster-sg from the Security Group and click [Review and Launch].  Step 7: click [Launch] on the Review Instance Launch screen. Create a key pair to access EC2 Instance. Select Create a new key pair, enter analytics-hol as the Key pair name, and click Download Key Pair. Save the Key Pair to any location on your PC and click [Launch Instances]. (EC2 Instance startup may take several minutes.)  For MacOS users, Change the File Permission of the downloaded Key Pair file to 400. $ chmod 400 ./analytics-hol.pem $ ls -lat analytics-hol.pem -r-------- 1 ****** ****** 1692 Jun 25 11:49 analytics-hol.pem   For Windows OS users, Please refer to Use PuTTY to connect to your Linux instance from Windows .\n"
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/lab-setup/ec2-user-configuration/",
	"title": "Configuring your EC2 Instance",
	"tags": [],
	"description": "",
	"content": "Configure the EC2 instances to access and control other AWS resources as follows:\n  Log into the EC2 instance by ssh.\nssh -i \u0026#34;\u0026lt;Key pair name\u0026gt;\u0026#34; ec2-user@\u0026lt;Public IP\u0026gt;   Perform the following actions in order on the EC2 instance connected with ssh.\n(1) Download the source code.\nwget \u0026#39;https://github.com/ksmin23/aws-analytics-immersion-day/archive/master.zip\u0026#39; (2) Extract the downloaded source code.\nunzip -u master.zip (3) Grant execution authority to the practice environment setting script.\nchmod +x ./aws-analytics-immersion-day-master/set-up-hands-on-lab.sh (4) Execute the setup script to set the lab environment.\n./aws-analytics-immersion-day-master/set-up-hands-on-lab.sh (5) Make sure the files necessary for the lab are normally created after running the configuration script. For example, check if the source code and necessary files exist as shown below.   Perform aws configure to access other AWS resources. At this time, the IAM User data created earlier is used. Open the previously downloaded .csv file, check the Access key ID and Secret access key, and enter them.\n$ aws configure AWS Access Key ID [None]: \u0026lt;Access key ID\u0026gt; AWS Secret Access Key [None]: \u0026lt;Secret access key\u0026gt; Default region name [None]: us-west-2 Default output format [None]:   If the setting is complete, the information entered as follows will be masked.\n$ aws configure AWS Access Key ID [****************EETA]: AWS Secret Access Key [****************CixY]: Default region name [None]: us-west-2 Default output format [None]:   "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/",
	"title": "Build up Data Analytics System",
	"tags": [],
	"description": "",
	"content": " In this lab, you will perform the following steps to build a data analysis system using the AWS Analytics services.\n  3-1. Create Kinesis Data Streams to receive input data  3-2. Create Kinesis Data Firehose to store data in S3  3-3. Verify data pipeline operation  3-4. Analyze data using Athena  3-5. Data visualization with QuickSight  3-6. (Optional) Combine small files stored in S3 into large files using AWS Lambda Function  3-7. Create Amazon Elasticsearch Service for Real-Time Data Analysis  3-8. Ingest real-time data into ElasticSearch using AWS Lambda Functions  3-8. Data visualization with Kibana   "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/kinesis-data-streams/",
	"title": "Kinesis Data Streams",
	"tags": [],
	"description": "",
	"content": "Create Kinesis Data Streams to receive input data Select Kinesis services the AWS Management Console.\n Click Get Started button. Click [Create data stream] button. Enter the desired name for Kinesis stream name (e.g. retail-trans). In Number of shards, enter the desired number of shards (e.g. 1). Click the [Create data stream] button and wait for the status of the created kinesis stream to become active.  "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/kinesis-data-firehose/",
	"title": "Kinesis Data Firehose",
	"tags": [],
	"description": "",
	"content": "Create Kinesis Data Firehose to store data in S3 With Kinesis Data Firehose, you can collect data in real time to destinations such as S3, Redshift, and ElasticSearch.\n Select Kinesis services the AWS Management Console.\n  Click Get Started button.\n  Click [Create delivery stream] on Deliver streaming data with Kinesis Firehose delivery streams menu.Start creating a new Firehose delivery stream.\n  (Step 1: Name and source) Enter the desired name (e.g. retail-trans) in Delivery stream name.\n  Next를 클릭합니다.Select Kinesis Data Stream from Choose a source, select Kinesis Data Stream (eg retail-trans) created earlier, and then click Next.\n  (Step 2: Process records) For Transform source records with AWS Lambda / Convert record format, both select the default option Disabled and click Next.\n  (Step 3: Choose a destination) Select Amazon S3 as Destination and click Create new to create an S3 bucket. The S3 bucket name is in the format of aws-analytics-immersion-day-xxxxxxxx in this lab, and xxxxxxxx is random numbers or characters so that the S3 bucket names do not overlap.\nEnter S3 prefix. For example, type as follows:\njson-data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/ Enter S3 error prefix. For example, type as follows:\nerror-json/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type} After entering S3 prefix and 3 error prefix, click Next. (reference: https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html )\n  (Step 4: Configure settings) Set buffer size to 1MB and buffer interval to 60 seconds in S3 buffer conditions.\n  Click the [Create new, or Choose] button in the IAM role below.\n  In the newly opened tab, it automatically creates the IAM role firehose_delivery_role with the required policy. Click Allow button to proceed.   After confirming that the newly created role has been added, click Next button.\n  (Step 5: Review) If there are no errors after checking the information entered in Review, click the [Create delivery stream] button to complete the Firehose creation.\n  "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/verify/",
	"title": "Verify",
	"tags": [],
	"description": "",
	"content": "Verify data pipeline operation Using the sample data, check whether data is normally collected in Kinesis Data Streams -\u0026gt; Kinesis Data Firehose -\u0026gt; S3.\n Connect SSH to the previously created E2 instance. Run gen_kinesis_data.py. $ python3 gen_kinesis_data.py --help usage: gen_kinesis_data.py [-h] [-I INPUT_FILE] [--out-format {csv,tsv,json}] [--service-name {kinesis,firehose}] [--stream-name STREAM_NAME] [--max-count MAX_COUNT] [--dry-run] optional arguments: -h, --help show this help message and exit -I INPUT_FILE, --input-file INPUT_FILE The input file path ex) ./resources/online_retail.csv --out-format {csv,tsv,json} --service-name {kinesis,firehose} --stream-name STREAM_NAME The name of the stream to put the data record into. --max-count MAX_COUNT The max number of records to put. --dry-run $ python3 gen_kinesis_data.py -I resources/online_retail.csv \\ --service-name kinesis \\ --out-format json \\ --stream-name retail-trans  Verify that data is generated every second. Proceed to the next step while running for sufficient data collection. If you check the S3 bucket you created a few minutes later, you can see that the original data generated is stored on S3 via Kinesis Data Firehose.  "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/athena/",
	"title": "Athena",
	"tags": [],
	"description": "",
	"content": "Analyze data using Athena Using Amazon Athena, you can create tables based on data stored in S3, query tables, and view query results. First, create a database to query the data.\nStep 1: Create a database  Open Athena console. The first time you visit Athena console, you will be taken to the [Get Started] page. Select [Get Started] to open the query editor. If this is your first time visiting, click set up a query result location in Amazon S3 to set the s3 location to save Athena\u0026rsquo;s query results. In this lab, we will create a directory to store Athena\u0026rsquo;s query results in the s3 bucket created in Kinesis Data Firehose setup step. For example, s3://aws-analytics-immersion-day-xxxxxxxx/athena-query-results/ (xxxxxxxx is random numbers or characters entered so that the bucket names do not overlap) Unless you are visiting for the first time, Athena Query Editor is oppened. You can see a query window with sample queries in the Athena Query Editor. Start typing your query anywhere in the query window. To create a database called mydatabase, enter the following CREATE DATABASE statement, then select [Run Query]. CREATE DATABASE mydatabase  Confirm that the the database [Catalog] refreshes and mydatabase is displayed in the [DATABASE] list on the left [Catalog] dashboard.  Step 2: Create a table  Make sure that mydatabase is selected in [DATABASE], and then select [New Query]. Enter the following CREATE TABLE statement in the query window and select [Run Query]. CREATE EXTERNAL TABLE `mydatabase.retail_trans_json`( `invoice` string COMMENT 'Invoice number', `stockcode` string COMMENT 'Product (item) code', `description` string COMMENT 'Product (item) name', `quantity` int COMMENT 'The quantities of each product (item) per transaction', `invoicedate` timestamp COMMENT 'Invoice date and time', `price` float COMMENT 'Unit price', `customer_id` string COMMENT 'Customer number', `country` string COMMENT 'Country name') PARTITIONED BY ( `year` int, `month` int, `day` int, `hour` int) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat' LOCATION 's3://aws-analytics-immersion-day-xxxxxxxx/json-data' The table retail_trans_json is created and displayed in the dashboard of the database [Catalog].\n After creating the table, select [New Query] and run the following to load the partition data. MSCK REPAIR TABLE mydatabase.retail_trans_json   Step 3: Query Data  Select [New Query]; enter the following query statement anywhere in the query window, then select [Run Query]. SELECT * FROM retail_trans_json LIMIT 10   The result is returned in the following format: "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/quicksight/",
	"title": "QuickSight",
	"tags": [],
	"description": "",
	"content": "Data visualization with QuickSight At this time, let\u0026rsquo;s use Amazon QuickSight to visualize the data.\n Go to QuickSight Console . Click the [Sign up for QuickSight] button to sign up for QuickSight. Select Standard Edition and click the [Continue] button. Specify the QuickSight account name randomly (in case of duplicate, the account will not be created), and enter your personal email address for Notification email address. QuckSight needs to access S3 so that click [Choose S3 buckets]. Select aws-analytics-immersion-day-xxxxxxxx and click [Finish]. After the account is created, click the [Go to Amazon QuickSight] button. After setting the region in the upper right corner as the region of the S3 bucket storing data, click [New Analysis] in the upper left corner. Click [New Data Set] button.  Click Athena and enter retail-quicksight in the Data source name in the pop-up window (any value can be entered). Click [Validate connection] to change to Validated, then click the [Create data source] button.  In the Choose your table screen, Database is mydatabase (the Athena database created earlier), and then Select retail_trans_json from Tables and click the Select button.  Click the [Visualize] button on the Finish data set creation screen. Check if the retail_trans_json table data is loaded into the QuickSight SPICE engine. Let\u0026rsquo;s visualize the Quantity and Price by ʻInvoicdDate. Select the ʻinvoicedate, price, and quantity fields in order from the left Fields list. Select vertical bar graph as Visual types.  Let\u0026rsquo;s share the Dashboard we just created with other users. Click the user icon in the upper left corner and click [Manage QuickSight]. Click the Invite users button, enter a random user account name (BI_user01), and click the [+] button on the right. Enter the email address of another user for Email, select AUTHOR for Role, and NO for IAM User, and click the Invite button.  Users receive the following Invitation Email and click Click to accept invitation to change their password in the Create Account menu.  Return to the QuickSight screen and click Share\u0026gt; Share analysis in the upper right.  Select BI_user01 and click the Share button.  Users receive the following email: You can check the analysis results by clicking [Click to View].   "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/athena-ctas/",
	"title": "Athena CTAS",
	"tags": [],
	"description": "",
	"content": "Combine small files stored in S3 into large files using AWS Lambda Function When real-time incoming data is stored in S3 using Kinesis Data Firehose, files with small data size are created. To improve the query performance of Amazon Athena, it is recommended to combine small files into one large file. To run these tasks periodically, I want to create an AWS Lambda function function that executes Athena\u0026rsquo;s Create Table As Select (CTAS) query.\n Open the AWS Lambda Console. Select [Create a function]. Enter MergeSmallFiles for Function name. Select Python 3.8 in Runtime. Select [Create a function].  Select [Add trigger] in the Designer tab. Select CloudWatch Events/EventBridge in Select a trigger of Trigger configuration. Select Create a new rule in Rule and enter the appropriate rule name (eg MergeSmallFilesEvent) in Rule name. Select Schedule expression as the rule type, and enter cron(5 * * * *) for running the task every 5 minutes in the schedule expression.  In Trigger configuration, click [Add]. Copy and paste the code from the athena_ctas.py file into the code editor of the Function code. Click [Add environment variables] to register the following environment variables. OLD_DATABASE=\u0026lt;source database\u0026gt; OLD_TABLE_NAME=\u0026lt;source table\u0026gt; NEW_DATABASE=\u0026lt;destination database\u0026gt; NEW_TABLE_NAME=\u0026lt;destination table\u0026gt; WORK_GROUP=\u0026lt;athena workgroup\u0026gt; OUTPUT_PREFIX=\u0026lt;destination s3 prefix\u0026gt; STAGING_OUTPUT_PREFIX=\u0026lt;staging s3 prefix used by athena\u0026gt; COLUMN_NAMES=\u0026lt;columns of source table excluding partition keys\u0026gt; For example, set Environment variables as follows:\nOLD_DATABASE=mydatabase OLD_TABLE_NAME=retail_trans_json NEW_DATABASE=mydatabase NEW_TABLE_NAME=ctas_retail_trans_parquet WORK_GROUP=primary OUTPUT_PREFIX=s3://aws-analytics-immersion-day-xxxxxxxx/parquet-retail-trans STAGING_OUTPUT_PREFIX=s3://aws-analytics-immersion-day-xxxxxxxx/tmp COLUMN_NAMES=invoice,stockcode,description,quantity,invoicedate,price,customer_id,country  To add the IAM Policy required to execute Athena queries, click View the MergeSmallFiles-role-XXXXXXXX role on the IAM console. in the Execution role and modify the IAM Role.  After clicking the [Attach policies] button in the [Permissions] tab of IAM Role, add AmazonAthenaFullAccess and AmazonS3FullAccess in order.  Select [Edit] in Basic settings. Adjust Memory and Timeout appropriately. In this lab, we set Timout to 5 min.  "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/amazon-es/",
	"title": "Amazon Elasticsearch Service",
	"tags": [],
	"description": "",
	"content": "Create Amazon Elasticsearch Service for Real-Time Data Analysis An Elasticsearch cluster is created to store and analyze data in real time. Amazon ES domains are synonymous with Elasticsearch clusters. A domain is a setting that specifies a setting, instance type, number of instances, and storage resources.\n In the AWS Management Console, select the Elasticsearch service in the Analytics category. (Step 1: Choose deployment type) Select Create a new domain. On the Create Elasticsearch domain page, select Production for Deployment type. For Version, choose the Elasticsearch version for your domain. We recommend that you choose the latest supported version. For more information, see Supported Elasticsearch Versions . Click [Next]. (Step 2: Configure domain) Enter the name of the domain. In this lab, retail will be used as the example domain name. For Instance type, choose the instance type of your Amazon ES domain. In this lab, it is recommended to use a small, economical instance type (t2.medium.elasticsearch) suitable for testing purposes. Enter the desired number of instances in Number of nodes. In this lab, we will use the default value of 1. Select EBS for Data nodes storage type.  a. Select General Purpose (SSD) for the EBS volume type. For more information, see Amazon EBS Volume Types. b. In EBS volume size, enter the EBS storage size per node for each data node in GiB. In this lab, we will use the default value of 10.   For now, you can ignore the Dedicated master nodes, Snapshot configuration and Optional Elasticsearch cluster settings sections. Click [Next]. (Step 3: Configure access and security) For Network configuration, select VPC access. Choose the appropriate VPC and subnet. Select the es-cluster-sg created in the preparation step as Security Groups. For now, disable Amazon Cognito Authentication and Fine–grained access control. For Access policy, select JSON defined access policy from Domain access policy, and then create and enter a JSON defined access policy using the following template in Add or edit the access policy.  JSON defined access policy Template - Enter the domain name entered in (Step 2: Configure domain) in \u0026lt;DOMAIN-NAME\u0026gt;. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;*\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;es:Describe*\u0026#34;, \u0026#34;es:List*\u0026#34;, \u0026#34;es:Get*\u0026#34;, \u0026#34;es:ESHttp*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:es:\u0026lt;region-id\u0026gt;:\u0026lt;account-id\u0026gt;:domain/\u0026lt;DOMAIN-NAME\u0026gt;/*\u0026#34; } ] }  ex) In this lab, we used retail as the domain name, so we create a JSON defined access policy as shown below. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;*\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;es:Describe*\u0026#34;, \u0026#34;es:List*\u0026#34;, \u0026#34;es:Get*\u0026#34;, \u0026#34;es:ESHttp*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:es:us-west-2:109624076471:domain/retail/*\u0026#34; } ] }    Encryption only allows Require HTTPS for all traffic to the domain, and other items are disabled. Keep all default values ​​of Encryption. Select [Next]. On the Review page, review your domain configuration and then choose Confirm.  "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/aws-lambda/",
	"title": "AWS Lambda",
	"tags": [],
	"description": "",
	"content": "Ingest real-time data into ElasticSearch using AWS Lambda Functions You can index data into Amazon Elasticsearch Service in real time using a Lambda function. In this lab, you will create a Lambda function using the AWS Lambda console.\nTo add a common library to Layers for use by Lambda functions,  Open the AWS Lambda Console. Enter the Layers menu and select [Create layer]. Enter es-lib for the Name. Select Upload a file from Amazon S3 and enter the s3 link url where the library code is stored or the compressed library code file. In this lab, we will use the resources/es-lib.zip file. For how to create es-lib.zip, refer to Example of creating a Python package to register in AWS Lambda Layer . Select Python 3.8 from Compatible runtimes.   To create a Lambda function,  Open the AWS Lambda Console. Select [Create a function]. Enter UpsertToES for Function name. Select Python 3.8 in Runtime. Select [Create a function].  Select [Add trigger] in the Designer tab. In Layers, choose Add a layer. Click Select from list of runtime compatible layers in Layer selection, and select Name and Version of the previously created layer as Name and Version in Compatiable layers. If the runtime version of Layer and the runtime of Lambda function are different, the layers required for Compatiable layers of Layer selection may not be displayed in the list. In this case, click Provide a layer version ARN in Layer selection, and enter arn of Layer directly.  Click [Add]. Select UpsertToES in the Designer tab to return to Function code and Configuration. Copy and paste the code from the upsert_to_es.py file into the code editor of the Function code. In Environment variables, click [Edit]. Click [Add environment variables] to register the following 4 environment variables. ES_HOST=\u0026lt;elasticsearch service domain\u0026gt; REQUIRED_FIELDS=\u0026lt;primary key로 사용될 column 목록\u0026gt; REGION_NAME=\u0026lt;region-name\u0026gt; DATE_TYPE_FIELDS=\u0026lt;column 중, date 또는 timestamp 데이터 타입의 column\u0026gt; For example, set Environment variables as follows:\nES_HOST=vpc-retail-xkl5jpog76d5abzhg4kyfilymq.us-west-1.es.amazonaws.com REQUIRED_FIELDS=Invoice,StockCode,Customer_ID REGION_NAME=us-west-2 DATE_TYPE_FIELDS=InvoiceDate  Click [Save]. In order to execute the lambda function in the VPC and read data from Kinesis Data Streams, you need to add the IAM Policy required for the Execution role required to execute the lamba function. Click View the UpsertToES-role-XXXXXXXX role on the IAM console. to edit the IAM Role.  After clicking the [Attach policies] button in the [Permissions] tab of IAM Role, add AWSLambdaVPCAccessExecutionRole and AmazonKinesisReadOnlyAccess in order.  Click the [Edit] button in the VPC category to go to the Edit VPC screen. Select Custom VPC for VPC connection. Choose the VPC and subnets where you created the domain for the Elasticsearch service, and choose the security groups that are allowed access to the Elasticsearch service domain. Select [Edit] in Basic settings. Adjust Memory and Timeout appropriately. In this lab, we set Timout to 5 min. Go back to the Designer tab and select [Add trigger]. Select Kinesis from Select a trigger in the Trigger configuration. Select the Kinesis Data Stream (retail-trans) created earlier in Kinesis stream. Click [Add].   "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/build-analytics-system/kibana/",
	"title": "Kibana",
	"tags": [],
	"description": "",
	"content": "Data visualization with Kibana Visualize data collected from Amazon Elasticsearch Service using Kibana.\n To access the Elasticsearch Cluster, add the ssh tunnel configuration to the ssh config file of the personal local PC as follows. # Elasticsearch Tunnel Host estunnel HostName \u0026lt;EC2 Public IP of Bastion Host\u0026gt; User ec2-user IdentitiesOnly yes IdentityFile ~/.ssh/analytics-hol.pem LocalForward 9200 \u0026lt;Elasticsearch Endpoint\u0026gt;:443    EC2 Public IP of Bastion Host uses the public IP of the EC2 instance created in the Lab setup step. ex) ~$ ls -1 .ssh/ analytics-hol.pem config id_rsa ~$ tail .ssh/config # Elasticsearch Tunnel Host estunnel HostName 214.132.71.219 User ubuntu IdentitiesOnly yes IdentityFile ~/.ssh/analytics-hol.pem LocalForward 9200 vpc-retail-qvwlxanar255vswqna37p2l2cy.us-west-2.es.amazonaws.com:443 ~$   Run ssh -N estunnel in Terminal. Connect to https://localhost:9200/_plugin/kibana/ in a web browser. (Home) Click [Use Elasticsearch data / Connect to your Elasticsearch index] in Add Data to Kibana.  (Management / Create index pattern) In Step 1 of 2: Define index pattern of Create index pattern, enter retail* in Index pattern.  (Management / Create index pattern) Choose [\u0026gt; Next step]. (Management / Create index pattern) Select InvoiceDate for the Time Filter field name in Step 2 of 2: Configure settings of the Create index pattern.  (Management / Create index pattern) Click [Create index pattern].  (Discover) After completing the creation of Index pattern, select Discover to check the data collected in Elasticsearch.  (Discover) Let\u0026rsquo;s visualize the Quantity by InvoicdDate. Select invoicdDate from Available fields on the left, and click Visualize at the bottom.  (Visualize) After selecting Y-Axis in Metrics on the Data tab, apply Sum for Aggregation, and Quantity for Field as shown below.  (Visualize) Click [Save] in the upper left corner, write down the name of the graph you saved, and then click [Confirm Save].  (Dashboards) Click Dashboard icon on the left and click the [Create new dashboard] button.  (Dashboards) Click [Add] on the upper left, and select the graph created in the previous step in Add Panels.  (Dashboards) Click [Save] at the top left, enter Title in the Save dashboard, and click [Confirm Save].  (Dashboards) You can see the following Dashboards.   "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/recap/",
	"title": "Recap and Review",
	"tags": [],
	"description": "",
	"content": " At the end of this lab, you should delete the resources you used to avoid incurring additional charges for the AWS account you used.\n Through this lab, We have built a Business Intelligent System with Lambda Architecture such that consists of real-time data processing and batch data processing layers.\n"
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/resources/",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": " https://github.com/ksmin23/aws-analytics-immersion-day  https://www.slideshare.net/ksmin23/aws-analytics-immersion-day-build-bi-system-from-scratch-230492367   "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/reference/",
	"title": "Reference",
	"tags": [],
	"description": "",
	"content": "AWS Developer Guide By Services  Amazon Simple Storage Service (Amazon S3)  Amazon Athena  Amazon Elasticsearch Service  AWS Lambda  Amazon Kinesis Data Firehose  Amazon Kinesis Data Streams  Amazon QuickSight  AWS Lambda Layers  Example of creating a Python package to register with AWS Lambda layer: elasticsearch  $ python3 -m venv es-lib\r$ cd es-lib\r$ source bin/activate\r$ mkdir -p python_modules\r$ pip install elasticsearch -t python_modules\r$ mv python_modules python\r$ zip -r es-lib.zip python/\r$ aws s3 cp es-lib.zip s3://my-lambda-layer-packages/python/\r  Further readings Amazon S3  New – Automatic Cost Optimization for Amazon S3 via Intelligent Tiering   Amazon Athena  Top 10 Performance Tuning Tips for Amazon Athena  Extract, Transform and Load data into S3 data lake using CTAS and INSERT INTO statements in Amazon Athena  Query Amazon S3 analytics data with Amazon Athena   Amazon Elasticsearch Service  Elasticsearch tutorial: a quick start guide  Run a petabyte scale cluster in Amazon Elasticsearch Service  Analyze user behavior using Amazon Elasticsearch Service, Amazon Kinesis Data Firehose and Kibana   AWS Lambda  Introduction to Messaging for Modern Cloud Architecture  Understanding the Different Ways to Invoke Lambda Functions   Amazon Kinesis Data Firehose  Amazon Kinesis Data Firehose custom prefixes for Amazon S3 objects  Amazon Kinesis Firehose Data Transformation with AWS Lambda   Amazon Kinesis Data Streams  Under the hood: Scaling your Kinesis data streams  Scale Amazon Kinesis Data Streams with AWS Application Auto Scaling   Amazon Kinesis Data Analytics  Streaming ETL with Apache Flink and Amazon Kinesis Data Analytics   Amazon QuickSight  10 visualizations to try in Amazon QuickSight with sample data  Visualize over 200 years of global climate data using Amazon Athena and Amazon QuickSight  Advanced analytics with table calculations in Amazon QuickSight   Etc  Optimize downstream data processing with Amazon Kinesis Data Firehose and Amazon EMR running Apache Spark  Serverless Scaling for Ingesting, Aggregating, and Visualizing Apache Logs with Amazon Kinesis Firehose, AWS Lambda, and Amazon Elasticsearch Service  Analyze Apache Parquet optimized data using Amazon Kinesis Data Firehose, Amazon Athena, and Amazon Redshift  Our data lake story: How Woot.com built a serverless data lake on AWS   "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/appendix/",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": " At the end of this lab, you should delete the resources you used to avoid incurring additional charges for the AWS account you used.\n Introducing how to deploy using the AWS CDK.\nPrerequisites   Install AWS CDK Toolkit.\nnpm install -g aws-cdk\r  Verify that cdk is installed properly by running the following command:\ncdk --version\rex)\n$ cdk --version\r1.51.0 (build 8c2d53c)\r  Useful commands  cdk ls list all stacks in the app cdk synth emits the synthesized CloudFormation template cdk deploy deploy this stack to your default AWS account/region cdk diff compare deployed stack with current state cdk docs open CDK documentation  Deployment When deployed as CDK, 1(a), 1(b), 1(c), 1(f), 2(b), 2(a) in the architecture diagram below are automatically created.\n  Refer to Getting Started With the AWS CDK to install cdk. Create an IAM User to be used when running cdk and register it in ~/.aws/config.For example, after creating an IAM User called cdk_user, add it to ~/.aws/config as shown below.\n$ cat ~/.aws/config\r[profile cdk_user]\raws_access_key_id=AKIAI44QH8DHBEXAMPLE\raws_secret_access_key=je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY\rregion=us-east-1\r  Create a Python package to register in the Lambda Layer and store it in the s3 bucket. For example, create an s3 bucket named lambda-layer-resources so that you can save the elasticsearch package to register in the Lambda Layer as follows.\n$ aws s3 ls s3://lambda-layer-resources/var/\r2019-10-25 08:38:50 0\r2019-10-25 08:40:28 1294387 es-lib.zip\r  After downloading the source code from git, enter the s3 bucket name where the package to be registered in the lambda layer is stored in an environment variable called S3_BUCKET_LAMBDA_LAYER_LIB. After setting, deploy using the cdk deploy command.\n$ git clone https://github.com/ksmin23/aws-analytics-immersion-day.git\r$ cd aws-analytics-immersion-day\r$ python3 -m venv .env\r$ source .env/bin/activate\r(.env) $ pip install -r requirements.txt\r(.env) $ S3_BUCKET_LAMBDA_LAYER_LIB=lambda-layer-resources cdk --profile cdk_user deploy\r  To delete the deployed application, execute the cdk destroy command as follows.\n(.env) $ cdk --profile cdk_user destroy\r  "
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/credits/",
	"title": "Credit",
	"tags": [],
	"description": "",
	"content": "Packages and Libraries  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support  Tools  Netlify - Continuous deployement and hosting of this documentation Hugo   Author: Sungmin Kim\n"
},
{
	"uri": "https://ksmin23.github.io/analytics-on-aws/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]